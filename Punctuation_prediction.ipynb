{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    " This assignment trained bi-directional  LSTM to learn how to automatically punctuate a sentence. The set of operation it learns include: comma, period and question mark.\n",
    " \n",
    "# Performance\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.95      0.94    264993\n",
    "           1       0.26      0.23      0.24     13813\n",
    "           2       0.41      0.36      0.39     19383\n",
    "           3       0.15      0.06      0.08      1331\n",
    "\n",
    "    accuracy                            0.87    299520\n",
    "    macro avg       0.44      0.40      0.41    299520\n",
    "    weighted avg    0.86      0.87      0.87    299520\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight, compute_sample_weight\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "LSTM_CHECKPOINT_NAME = 'checkpoint/2lstm_epoch25_chunck80_file350.h5'\n",
    "BLSTM_CHECKPOINT_NAME = 'checkpoint/2blstm_epoch25_chunck80_file350.h5'\n",
    "TOKENIZER_NAME = 'pickle/tokenizer_chunck80_file430.pickle'\n",
    "TRAINING_SPLIT = 0.8\n",
    "MAX_SEQUENCE_LENGTH = 80\n",
    "MAX_NUM_WORDS = 20000\n",
    "NUM_FILES = 350\n",
    "NUM_EPOCH = 25\n",
    "BATCH_SIZES = 128\n",
    "FILENAMES = glob.glob('input/*.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8805\n"
     ]
    }
   ],
   "source": [
    "paragraphs = []\n",
    "for filename in FILENAMES[:NUM_FILES]:\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    for wordElement in root.iter('post'):\n",
    "        text = wordElement.text.lower()\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r'[.]+', \".\", text)\n",
    "        text = re.sub(r'[?]+', \"?\", text)\n",
    "        text = re.sub(r'[!]+', \".\", text)\n",
    "        text = re.sub(r'[:]+', \",\", text)\n",
    "        text = re.sub(r'[;]+', \",\", text)\n",
    "        text = re.sub(r'[^.,\\?a-zA-Z ]', '', text)\n",
    "        paragraphs.append(text)\n",
    "        #print(paragraphs)\n",
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497508\n"
     ]
    }
   ],
   "source": [
    "words_labels_join = []\n",
    "for paragraph in paragraphs:\n",
    "    words = paragraph.split()\n",
    "    for word in words:\n",
    "        if re.match(\"^[a-z]+$\", word):\n",
    "            words_labels_join.append((word, 0))\n",
    "        if re.match(\"^[a-z]+,$\", word):\n",
    "            words_labels_join.append((word[:-1], 1))\n",
    "        if re.match(\"^[a-z]+\\.$\", word):\n",
    "            words_labels_join.append((word[:-1], 2))\n",
    "        if re.match(\"^[a-z]+\\?$\", word):\n",
    "            words_labels_join.append((word[:-1] , 3))\n",
    "print(len(words_labels_join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_labels_chunk = [words_labels_join[i:i + MAX_SEQUENCE_LENGTH] for i in range(0, len(words_labels_join), MAX_SEQUENCE_LENGTH)]\n",
    "words_labels_chunk = words_labels_chunk[:-1]\n",
    "random.shuffle(words_labels_chunk)\n",
    "\n",
    "features = [[x[0] for x in sublist] for sublist in words_labels_chunk]\n",
    "labels = [[x[1] for x in sublist] for sublist in words_labels_chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=1)\n",
    "tokenizer.fit_on_texts(features)\n",
    "features_numeric = tokenizer.texts_to_sequences(features)\n",
    "\n",
    "with open(TOKENIZER_NAME, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(features_numeric)\n",
    "train_feature = features_numeric[:int(length*0.8)]\n",
    "test_feature = features_numeric[int(length*0.8):]\n",
    "train_label = labels[:int(length*0.8)]\n",
    "test_label = labels[int(length*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = to_categorical(np.asarray(train_label))\n",
    "test_label = to_categorical(np.asarray(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_model = Sequential()\n",
    "#lstm_model.add(InputLayer(input_shape=(MAX_SEQUENCE_LENGTH, )))\n",
    "#lstm_model.add(Embedding(MAX_NUM_WORDS+1, 128))\n",
    "#lstm_model.add(LSTM(64, return_sequences=True))\n",
    "#lstm_model.add(LSTM(64, return_sequences=True))\n",
    "#lstm_model.add(TimeDistributed(Dense(output_dim=4, activation='softmax')))\n",
    "#lstm_model.compile(loss='categorical_crossentropy',\n",
    "#          optimizer='adam',\n",
    "#          metrics=['categorical_accuracy'],\n",
    "#          sample_weight_mode='temporal')\n",
    "#lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_model.fit(np.array(train_feature), \n",
    "#               train_label, \n",
    "#               batch_size=128, \n",
    "#               epochs=25, \n",
    "#               validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transform(sequences, index):\n",
    "    label_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        label_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            label_sequence.append(index[np.argmax(categorical)])\n",
    "        label_sequences.append(label_sequence)\n",
    "    return label_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 80, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 80, 128)           98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 80, 4)             516       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 80, 4)             0         \n",
      "=================================================================\n",
      "Total params: 2,758,148\n",
      "Trainable params: 2,758,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "blstm_model = Sequential()\n",
    "blstm_model.add(InputLayer(input_shape=(MAX_SEQUENCE_LENGTH, )))\n",
    "blstm_model.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "blstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "blstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "#blstm_model.add(Dropout(0.2))\n",
    "blstm_model.add(TimeDistributed(Dense(4)))\n",
    "blstm_model.add(Activation('softmax'))\n",
    "blstm_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'],\n",
    "              sample_weight_mode='temporal')\n",
    " \n",
    "blstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11979 samples, validate on 2995 samples\n",
      "Epoch 1/25\n",
      "11979/11979 [==============================] - 57s 5ms/step - loss: 0.5450 - acc: 0.8782 - val_loss: 0.4568 - val_acc: 0.8829\n",
      "Epoch 2/25\n",
      "11979/11979 [==============================] - 51s 4ms/step - loss: 0.4273 - acc: 0.8849 - val_loss: 0.3834 - val_acc: 0.8828\n",
      "Epoch 3/25\n",
      "11979/11979 [==============================] - 54s 4ms/step - loss: 0.3331 - acc: 0.8890 - val_loss: 0.3221 - val_acc: 0.8902\n",
      "Epoch 4/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.2988 - acc: 0.8965 - val_loss: 0.3132 - val_acc: 0.8931\n",
      "Epoch 5/25\n",
      "11979/11979 [==============================] - 51s 4ms/step - loss: 0.2814 - acc: 0.9022 - val_loss: 0.3082 - val_acc: 0.8956\n",
      "Epoch 6/25\n",
      "11979/11979 [==============================] - 53s 4ms/step - loss: 0.2681 - acc: 0.9066 - val_loss: 0.3069 - val_acc: 0.8952\n",
      "Epoch 7/25\n",
      "11979/11979 [==============================] - 51s 4ms/step - loss: 0.2583 - acc: 0.9102 - val_loss: 0.3088 - val_acc: 0.8941\n",
      "Epoch 8/25\n",
      "11979/11979 [==============================] - 50s 4ms/step - loss: 0.2487 - acc: 0.9132 - val_loss: 0.3141 - val_acc: 0.8944\n",
      "Epoch 9/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.2397 - acc: 0.9164 - val_loss: 0.3187 - val_acc: 0.8934\n",
      "Epoch 10/25\n",
      "11979/11979 [==============================] - 50s 4ms/step - loss: 0.2287 - acc: 0.9199 - val_loss: 0.3248 - val_acc: 0.8887\n",
      "Epoch 11/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.2186 - acc: 0.9234 - val_loss: 0.3348 - val_acc: 0.8914\n",
      "Epoch 12/25\n",
      "11979/11979 [==============================] - 51s 4ms/step - loss: 0.2073 - acc: 0.9276 - val_loss: 0.3388 - val_acc: 0.8889\n",
      "Epoch 13/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.1970 - acc: 0.9314 - val_loss: 0.3523 - val_acc: 0.8862\n",
      "Epoch 14/25\n",
      "11979/11979 [==============================] - 50s 4ms/step - loss: 0.1877 - acc: 0.9349 - val_loss: 0.3691 - val_acc: 0.8821\n",
      "Epoch 15/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.1780 - acc: 0.9386 - val_loss: 0.3798 - val_acc: 0.8832\n",
      "Epoch 16/25\n",
      "11979/11979 [==============================] - 48s 4ms/step - loss: 0.1680 - acc: 0.9424 - val_loss: 0.3982 - val_acc: 0.8846\n",
      "Epoch 17/25\n",
      "11979/11979 [==============================] - 50s 4ms/step - loss: 0.1581 - acc: 0.9463 - val_loss: 0.4205 - val_acc: 0.8789\n",
      "Epoch 18/25\n",
      "11979/11979 [==============================] - 48s 4ms/step - loss: 0.1489 - acc: 0.9496 - val_loss: 0.4412 - val_acc: 0.8831\n",
      "Epoch 19/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.1393 - acc: 0.9531 - val_loss: 0.4642 - val_acc: 0.8800\n",
      "Epoch 20/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.1297 - acc: 0.9568 - val_loss: 0.4842 - val_acc: 0.8764\n",
      "Epoch 21/25\n",
      "11979/11979 [==============================] - 48s 4ms/step - loss: 0.1206 - acc: 0.9601 - val_loss: 0.5175 - val_acc: 0.8790\n",
      "Epoch 22/25\n",
      "11979/11979 [==============================] - 50s 4ms/step - loss: 0.1125 - acc: 0.9629 - val_loss: 0.5498 - val_acc: 0.8752\n",
      "Epoch 23/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.1038 - acc: 0.9660 - val_loss: 0.5819 - val_acc: 0.8736\n",
      "Epoch 24/25\n",
      "11979/11979 [==============================] - 50s 4ms/step - loss: 0.0967 - acc: 0.9684 - val_loss: 0.6302 - val_acc: 0.8717\n",
      "Epoch 25/25\n",
      "11979/11979 [==============================] - 49s 4ms/step - loss: 0.0909 - acc: 0.9705 - val_loss: 0.6448 - val_acc: 0.8733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10fba30f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm_model.fit(np.array(train_feature), \n",
    "          train_label, \n",
    "          batch_size=128, \n",
    "          epochs=NUM_EPOCH,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_model.save(BLSTM_CHECKPOINT_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3744/3744 [==============================] - 5s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "blstm_y_pred = blstm_model.predict(np.array(test_feature), batch_size=BATCH_SIZES, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94    264993\n",
      "           1       0.26      0.23      0.24     13813\n",
      "           2       0.41      0.36      0.39     19383\n",
      "           3       0.15      0.06      0.08      1331\n",
      "\n",
      "    accuracy                           0.87    299520\n",
      "   macro avg       0.44      0.40      0.41    299520\n",
      "weighted avg       0.86      0.87      0.87    299520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_index = {0:0, 1:1, 2:2, 3:3}\n",
    "text_label_trans = Transform(test_label, label_index)\n",
    "blstm_y_pred_trans = Transform(blstm_y_pred, label_index)\n",
    "print(classification_report(np.array(text_label_trans).flatten(), np.array(blstm_y_pred_trans).flatten(), labels=[0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "this is a string of text with no punctuation . this is a new sentence .\n"
     ]
    }
   ],
   "source": [
    "str_input = 'this is a string of text with no punctuation this is a new sentence' \n",
    "str_split = str_input.split()\n",
    "str_chunk = [str_split[i:i + MAX_SEQUENCE_LENGTH] for i in range(0, len(str_split), MAX_SEQUENCE_LENGTH)]\n",
    "str_numeric = np.array(tokenizer.texts_to_sequences(str_chunk))\n",
    "str_pad = pad_sequences(str_numeric, MAX_SEQUENCE_LENGTH, padding='post')\n",
    "blstm_str_pred = blstm_model.predict(str_pad, batch_size=64, verbose=1)\n",
    "blstm_str_trans = Transform(blstm_str_pred, label_index)\n",
    "\n",
    "result = []\n",
    "for row, chunk in enumerate(str_chunk):\n",
    "    for col, word in enumerate(chunk):\n",
    "        if blstm_str_trans[row][col] == 0:\n",
    "            result.append(word)\n",
    "        if blstm_str_trans[row][col] == 1:\n",
    "            result.append(word)\n",
    "            result.append('<comma>')\n",
    "        if blstm_str_trans[row][col] == 2:\n",
    "            result.append(word)\n",
    "            result.append('<period>')\n",
    "        if blstm_str_trans[row][col] == 3:\n",
    "            result.append(word)\n",
    "            result.append('<question_mark>')\n",
    "print(' '.join(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
